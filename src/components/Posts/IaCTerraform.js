import React, { useEffect } from "react";
import styled from "styled-components";

// helpers
import { Analytics } from "../../helpers/analytics";

// animations
import SlideInBottom from "../../animations/SlideInBottom";

// components
import BackButton from "../Button/BackButton";
import { CodeBlockWithCopy } from "../Code/Code";
import CodeCarousel from "../CodeCarousel/CodeCarousel";

// layout
import {
  PageWrapper,
  PostTopBar,
  PostContainer as BasePostContainer,
  HeaderRow,
  IconWrapper,
  HeaderIcon,
} from '../BlogLayout/BlogLayout';

// typography
import {
  PageTitle,
  SectionHeading,
  SubSectionHeading,
  Paragraph,
  Strong,
  TextLink,
  TextList,
  TextListItem,
  InlineHighlight,
  IndentedTextList,
  IndentedTextListItem,
  TertiaryHeading,
} from "../Typography/Typography";

// icons
import { TerraformSVG } from '../../resources/styles/icons';

const AnimatedPostContainer = styled(BasePostContainer)`
  animation: ${SlideInBottom} 0.5s forwards;
`;

const verifyTf = `terraform version`;

const terraformFolderTree = `template-terraform-boilerplate/
├─ .github/
│  └─ workflows/
│     └─ terraform.yml
├─ infra/
│  ├─ env/
│  │  └─ <aws-account>/
│  │     ├─ env.tfvars
│  │     ├─ main.tf
│  │     ├─ outputs.tf
│  │     ├─ providers.tf
│  │     ├─ variables.tf
│  │     └─ backend.tf
│  ├─ modules/
│  │  └─ <module-name>/
│  │     ├─ main.tf
│  │     ├─ outputs.tf
│  │     └─ variables.tf
│  └─ scripts/
│     ├─ prereqs.sh
│     ├─ whoami.sh
│     ├─ use-env.sh
│     ├─ bootstrap-state.sh
│     ├─ fmt.sh
│     ├─ validate.sh
│     ├─ plan.sh
│     ├─ write-backend-hcl.sh
│     └─ apply.sh
├─ .gitignore
├─ README.md
└─ package.json`;

const envMainTf = `/*
main.tf (environment root)
- This is the deployable entry point for an environment.
- Keep it readable: wire modules together, pass variables, expose outputs.
*/

module "example_bucket" {
  source = "../../modules/example-s3-bucket"

  project     = var.project
  environment = var.environment

  # Example input for the module
  bucket_suffix = "uploads"
}`;

const envVariablesTf = `/*
variables.tf (environment root)
- Defines the inputs this environment expects.
- Keep types + descriptions tight so usage is obvious and mistakes are caught early.
*/

variable "project" {
  type        = string
  description = "Project name used for naming/tagging."
}

variable "environment" {
  type        = string
  description = "A label for this deployable root (often matches the folder name under infra/env/)."
}

variable "aws_region" {
  type        = string
  description = "AWS region to deploy into."
  default     = "eu-west-2"
}`;


const envTfvars = `/*
env.tfvars (environment root)
- Values specific to this deployable root.
- Keeps main.tf identical across accounts/environments.
*/

project     = "template-terraform-boilerplate"
environment = "<aws-account>"
aws_region  = "eu-west-2"`;



const envProvidersTf = `/*
providers.tf (environment root)
- Configures provider(s) used by this environment.
- Makes the deployment context explicit (region/account/role).
- Providers are configured at the root and inherited by modules.
*/

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region

  default_tags {
    tags = {
      Project     = var.project
      Environment = var.environment
      ManagedBy   = "terraform"
    }
  }
}`;


const envOutputsTf = `/*
outputs.tf (environment root)
- Outputs are the values you want to quickly grab after apply.
- Think: URLs, IDs, ARNs, bucket names, etc.
*/

output "uploads_bucket_name" {
  description = "Name of the uploads S3 bucket."
  value       = module.example_bucket.bucket_name
}`;


const envBackendTf = `/*
backend.tf (environment root)
- Backend values cannot use variables, and you don't want account-specific state settings committed to Git.
- This template keeps backend.tf minimal and injects real values at init time using infra/backend.hcl (generated by the CLI).
*/

terraform {
  backend "s3" {}
}`;

const moduleMainTf = `/*
main.tf (module)
- Modules are reusable building blocks.
- Keep modules focused: one responsibility, clear inputs/outputs.
*/

resource "aws_s3_bucket" "this" {
  bucket = "\${var.project}-\${var.environment}-\${var.bucket_suffix}"
}`;

const moduleVariablesTf = `/*
variables.tf (module)
- Inputs required by the module.
- Keep them minimal and well-described.
*/

variable "project" {
  type        = string
  description = "Project name used for naming/tagging."
}

variable "environment" {
  type        = string
  description = "Environment name (aws-account)."
}

variable "bucket_suffix" {
  type        = string
  description = "Suffix used to build the bucket name (e.g., uploads)."
}`;

const moduleOutputsTf = `/*
outputs.tf (module)
- Outputs are how other parts of the system connect to this module.
*/

output "bucket_name" {
  description = "Name of the S3 bucket created by this module."
  value       = aws_s3_bucket.this.bucket
}`;

const scriptFmt = `#!/usr/bin/env bash
set -euo pipefail

# fmt.sh
# - Formats Terraform code under infra/ recursively.
# - This modifies files (not a check-only).
#
# Usage:
#   bash infra/scripts/fmt.sh

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

terraform fmt -recursive
echo "fmt complete"`;


const scriptValidate = `#!/usr/bin/env bash
set -euo pipefail

# validate.sh
# - Local/CI quality gate for an environment under infra/env/<environment>
# - Includes:
#   1) terraform fmt (writes changes)
#   2) terraform validate (syntax + internal consistency)
#   3) tflint (recursive linting)
#
# Usage:
#   bash infra/scripts/validate.sh <environment>

ENVIRONMENT="\${1:-}"
if [ -z "$ENVIRONMENT" ]; then
  echo "Usage: bash infra/scripts/validate.sh <environment>"
  exit 1
fi

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
ENV_DIR="$ROOT_DIR/env/$ENVIRONMENT"

if [ ! -d "$ENV_DIR" ]; then
  echo "Environment folder not found: $ENV_DIR"
  exit 1
fi

echo "Validate (fmt → terraform validate → tflint)"
echo "Environment: $ENVIRONMENT"
echo ""

echo "→ terraform fmt"
bash "$ROOT_DIR/scripts/fmt.sh"
echo ""

echo "→ terraform validate"
cd "$ENV_DIR"
terraform init -backend=false -input=false >/dev/null
terraform validate
echo "terraform validate passed"
echo ""

# tflint: required in CI, optional locally (but if present we run it).
echo "→ tflint"
if ! command -v tflint >/dev/null 2>&1; then
  echo "tflint is not installed"
  echo "Install: https://github.com/terraform-linters/tflint"
  exit 1
fi

cd "$ROOT_DIR"
tflint --recursive
echo "tflint passed"
echo ""

echo "validate complete for environment: $ENVIRONMENT"`;

const scriptPlan = `#!/usr/bin/env bash
set -euo pipefail

# plan.sh
# - Creates a plan for a chosen deployable root under infra/env/<environment>.
# - Uses env.tfvars to supply values.
# - Outputs a tfplan file so apply uses an exact, reviewed plan.
#
# Usage:
#   bash infra/scripts/plan.sh <environment>

ENVIRONMENT="\${1:-}"
if [ -z "$ENVIRONMENT" ]; then
  echo "Usage: bash infra/scripts/plan.sh <environment>"
  exit 1
fi

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
ENV_DIR="$ROOT_DIR/env/$ENVIRONMENT"

if [ ! -d "$ENV_DIR" ]; then
  echo "Environment folder not found: $ENV_DIR"
  exit 1
fi

echo "Plan"
echo "Environment: $ENVIRONMENT"
echo ""

cd "$ENV_DIR"

terraform init -input=false -backend-config=backend.hcl

terraform plan -input=false \
  -var-file="env.tfvars" \
  -out="tfplan"

echo "plan complete for environment: $ENVIRONMENT"
echo "Plan saved to: $ENV_DIR/tfplan"`;

const scriptApply = `#!/usr/bin/env bash
set -euo pipefail

# apply.sh
# - Applies a previously generated plan file (tfplan).
#
# Usage:
#   bash infra/scripts/apply.sh <environment>

ENVIRONMENT="\${1:-}"
if [ -z "$ENVIRONMENT" ]; then
  echo "Usage: bash infra/scripts/apply.sh <environment>"
  exit 1
fi

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
ENV_DIR="$ROOT_DIR/env/$ENVIRONMENT"

if [ ! -d "$ENV_DIR" ]; then
  echo "Environment folder not found: $ENV_DIR"
  exit 1
fi

echo "Apply"
echo "Environment: $ENVIRONMENT"
echo ""

cd "$ENV_DIR"

if [ ! -f "tfplan" ]; then
  echo "No tfplan found in $ENV_DIR"
  echo "Run: bash infra/scripts/plan.sh $ENVIRONMENT"
  exit 1
fi

terraform apply -input=false "tfplan"
echo "apply complete for environment: $ENVIRONMENT"`;

const scriptPreReqs = `#!/usr/bin/env bash
set -euo pipefail

# prereqs.sh
# - Verifies required tooling is available *in this shell* (PATH).
# - Does not install anything; fails fast with a clear hint.
#
# Usage:
#   bash infra/scripts/prereqs.sh

need() {
  local bin="$1"
  local hint="$2"

  if ! command -v "$bin" >/dev/null 2>&1; then
    echo ""
    echo "Missing: $bin"
    echo "Fix: $hint"
    echo ""
    exit 1
  fi
}

echo "Checking prerequisites..."
echo ""

need terraform "Install Terraform and ensure it's on PATH"
need aws       "Install AWS CLI v2 and ensure it's on PATH"
need jq        "Install jq and ensure it's on PATH (optional for pretty output, but required by this template)"
need node      "Install Node.js (LTS) and ensure it's on PATH"
need npm       "npm should come with Node.js (LTS)"

# Optional locally. CI installs it in the workflow.
if ! command -v tflint >/dev/null 2>&1; then
  echo "Note: tflint not found (optional locally; CI installs it in the workflow)"
  echo "Install: https://github.com/terraform-linters/tflint"
  echo ""
fi

echo "All required tools are available."
echo ""

echo "terraform: $(terraform version | head -n 1)"
echo "aws:       $(aws --version 2>&1)"
echo "jq:        $(jq --version)"
echo "node:      $(node --version)"
echo "npm:       $(npm --version)"

if command -v tflint >/dev/null 2>&1; then
  echo "tflint:    $(tflint --version | head -n 1)"
fi

echo ""`;

const scriptBootstrapState = `#!/usr/bin/env bash
set -euo pipefail

# bootstrap-state.sh
# - Creates Terraform remote state prerequisites in the CURRENT AWS account:
#   - S3 bucket for state (versioning + encryption + public access block)
#   - DynamoDB table for state locking
# - Sets up GitHub Actions OIDC:
#   - IAM OIDC provider for token.actions.githubusercontent.com (idempotent)
#   - GitHub OIDC role that Actions assumes
# - Writes backend config to:
#   infra/env/<environment>/backend.hcl (via write-backend-hcl.sh)
#
# Usage (Git Bash, from repo root):
#   source infra/scripts/use-env.sh <environment>
#   bash infra/scripts/bootstrap-state.sh <environment> --region eu-west-2
#
# Notes:
# - This script creates resources in whichever AWS account your current auth points to.
# - Always check the printed Account + Caller ARN before continuing.

usage() {
  echo "Usage: bash infra/scripts/bootstrap-state.sh <environment> [--region eu-west-2] [--github-role-name GitHubOIDCTerraformRole] [--github-repo owner/repo] [--project-name template-terraform-boilerplate]"
  exit 1
}

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

ENVIRONMENT="\${1:-}"
shift || true
if [ -z "$ENVIRONMENT" ]; then
  usage
fi

REGION="\${AWS_REGION:-eu-west-2}"
GITHUB_ROLE_NAME="GitHubOIDCTerraformRole"
GITHUB_REPO=""
PROJECT_NAME="template-terraform-boilerplate"

while [ "\${1:-}" != "" ]; do
  case "$1" in
    --region)
      shift
      REGION="\${1:-}"
      ;;
    --github-role-name)
      shift
      GITHUB_ROLE_NAME="\${1:-}"
      ;;
    --github-repo)
      shift
      GITHUB_REPO="\${1:-}"
      ;;
    --project-name)
      shift
      PROJECT_NAME="\${1:-}"
      ;;
    *)
      echo "Unknown arg: $1"
      usage
      ;;
  esac
  shift || true
done

ENV_DIR="$ROOT_DIR/env/$ENVIRONMENT"
if [ ! -d "$ENV_DIR" ]; then
  echo "Environment folder not found: $ENV_DIR"
  echo "Create it under infra/env/ and try again."
  exit 1
fi

if ! command -v aws >/dev/null 2>&1; then
  echo "AWS CLI is required."
  echo "Run: bash infra/scripts/prereqs.sh"
  exit 1
fi

ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text 2>/dev/null || true)"
CALLER_ARN="$(aws sts get-caller-identity --query Arn --output text 2>/dev/null || true)"

if [ -z "$ACCOUNT_ID" ] || [ "$ACCOUNT_ID" = "None" ] || [ -z "$CALLER_ARN" ] || [ "$CALLER_ARN" = "None" ]; then
  echo "Could not determine AWS account/principal. Are you authenticated?"
  echo "Tip: set AWS_PROFILE and run: aws sts get-caller-identity"
  exit 1
fi

# Detect owner/repo from git remote if not provided
if [ -z "$GITHUB_REPO" ] && command -v git >/dev/null 2>&1; then
  ORIGIN_URL="$(git config --get remote.origin.url 2>/dev/null || true)"
  if echo "$ORIGIN_URL" | grep -q "github.com"; then
    GITHUB_REPO="$(echo "$ORIGIN_URL" \
      | sed -E 's#^git@github\.com:##' \
      | sed -E 's#^https://github\.com/##' \
      | sed -E 's#\.git$##')"
  fi
fi

if [ -z "$GITHUB_REPO" ]; then
  echo "Could not auto-detect the GitHub repo."
  echo "Fix: re-run with --github-repo owner/repo"
  exit 1
fi

STATE_BUCKET="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tfstate"
LOCK_TABLE="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tflocks"

OIDC_URL="https://token.actions.githubusercontent.com"
OIDC_PROVIDER_ARN="arn:aws:iam::\${ACCOUNT_ID}:oidc-provider/token.actions.githubusercontent.com"

echo ""
echo "Bootstrap (remote state + GitHub OIDC)"
echo "Environment:   $ENVIRONMENT"
echo "Account:       $ACCOUNT_ID"
echo "Region:        $REGION"
echo "Caller ARN:    $CALLER_ARN"
echo "GitHub repo:   $GITHUB_REPO"
echo "State bucket:  $STATE_BUCKET"
echo "Lock table:    $LOCK_TABLE"
echo "GitHub role:   $GITHUB_ROLE_NAME"
echo ""

echo "→ Ensuring S3 bucket exists..."
if aws s3api head-bucket --bucket "$STATE_BUCKET" >/dev/null 2>&1; then
  echo "  - Bucket exists: $STATE_BUCKET"
else
  if [ "$REGION" = "us-east-1" ]; then
    aws s3api create-bucket --bucket "$STATE_BUCKET" >/dev/null
  else
    aws s3api create-bucket \
      --bucket "$STATE_BUCKET" \
      --create-bucket-configuration "LocationConstraint=$REGION" >/dev/null
  fi
  echo "  - Bucket created: $STATE_BUCKET"
fi

echo "→ Configuring bucket (versioning, encryption, public access block)..."
aws s3api put-bucket-versioning \
  --bucket "$STATE_BUCKET" \
  --versioning-configuration Status=Enabled >/dev/null

aws s3api put-bucket-encryption \
  --bucket "$STATE_BUCKET" \
  --server-side-encryption-configuration '{
    "Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]
  }' >/dev/null

aws s3api put-public-access-block \
  --bucket "$STATE_BUCKET" \
  --public-access-block-configuration '{
    "BlockPublicAcls":true,
    "IgnorePublicAcls":true,
    "BlockPublicPolicy":true,
    "RestrictPublicBuckets":true
  }' >/dev/null

echo "  - Bucket configured"

echo ""
echo "→ Ensuring DynamoDB lock table exists..."
if aws dynamodb describe-table --table-name "$LOCK_TABLE" >/dev/null 2>&1; then
  echo "  - Table exists: $LOCK_TABLE"
else
  aws dynamodb create-table \
    --table-name "$LOCK_TABLE" \
    --attribute-definitions AttributeName=LockID,AttributeType=S \
    --key-schema AttributeName=LockID,KeyType=HASH \
    --billing-mode PAY_PER_REQUEST >/dev/null

  echo "  - Table created: $LOCK_TABLE"
  echo "  - Waiting for table to become ACTIVE..."
  aws dynamodb wait table-exists --table-name "$LOCK_TABLE"
fi

echo ""
echo "→ Ensuring GitHub OIDC provider exists..."
if aws iam get-open-id-connect-provider --open-id-connect-provider-arn "$OIDC_PROVIDER_ARN" >/dev/null 2>&1; then
  echo "  - OIDC provider exists"
else
  THUMBPRINT="6938fd4d98bab03faadb97b34396831e3780aea1"
  aws iam create-open-id-connect-provider \
    --url "$OIDC_URL" \
    --client-id-list "sts.amazonaws.com" \
    --thumbprint-list "$THUMBPRINT" >/dev/null
  echo "  - OIDC provider created"
fi

echo "→ Ensuring GitHub OIDC role exists..."
if aws iam get-role --role-name "$GITHUB_ROLE_NAME" >/dev/null 2>&1; then
  echo "  - Role exists: $GITHUB_ROLE_NAME"
else
  GITHUB_TRUST_POLICY="$(cat <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowGitHubActionsOIDC",
      "Effect": "Allow",
      "Principal": { "Federated": "\${OIDC_PROVIDER_ARN}" },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
        },
        "StringLike": {
          "token.actions.githubusercontent.com:sub": "repo:\${GITHUB_REPO}:*"
        }
      }
    }
  ]
}
EOF
)"
  aws iam create-role \
    --role-name "$GITHUB_ROLE_NAME" \
    --assume-role-policy-document "$GITHUB_TRUST_POLICY" >/dev/null
  echo "  - Role created: $GITHUB_ROLE_NAME"
fi

GITHUB_ROLE_ARN="$(aws iam get-role --role-name "$GITHUB_ROLE_NAME" --query 'Role.Arn' --output text 2>/dev/null || true)"
if [ -z "$GITHUB_ROLE_ARN" ] || [ "$GITHUB_ROLE_ARN" = "None" ]; then
  echo "Could not resolve GitHub role ARN for: $GITHUB_ROLE_NAME"
  exit 1
fi

echo "→ Ensuring GitHub role has permissions (AdministratorAccess)..."
aws iam attach-role-policy \
  --role-name "$GITHUB_ROLE_NAME" \
  --policy-arn "arn:aws:iam::aws:policy/AdministratorAccess" >/dev/null || true

echo ""
echo "→ Writing backend.hcl for $ENVIRONMENT..."
bash "$ROOT_DIR/scripts/write-backend-hcl.sh" "$ENVIRONMENT" --region "$REGION" --project-name "$PROJECT_NAME"

echo ""
echo "Bootstrap complete"
echo ""
echo "Next (local):"
echo "  source infra/scripts/use-env.sh $ENVIRONMENT"
echo "  bash infra/scripts/whoami.sh"
echo "  cd infra/env/$ENVIRONMENT"
echo "  terraform init -backend-config=backend.hcl"
echo ""
echo "Next (GitHub):"
echo "  Create GitHub Environment named: $ENVIRONMENT"
echo "  Add Environment secret AWS_ROLE_ARN = $GITHUB_ROLE_ARN"
echo ""`;

const scriptWriteBackendHCL = `#!/usr/bin/env bash
set -euo pipefail

# write-backend-hcl.sh
# - Writes an env-bound write-backend.hcl at:
#   infra/env/<environment>/write-backend.hcl
# - Uses the current AWS identity to derive the account ID.
# - Matches the same naming convention used by bootstrap-state.sh.
#
# Usage:
#   bash infra/scripts/write-backend-hcl.sh <environment> [--region eu-west-2] [--project-name template-terraform-boilerplate]
#
# Notes:
# - write-backend.hcl is intentionally NOT committed to git.

usage() {
  echo "Usage: bash infra/scripts/write-backend-hcl.sh <environment> [--region eu-west-2] [--project-name template-terraform-boilerplate]"
  exit 1
}

ROOT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

ENVIRONMENT="\${1:-}"
shift || true
if [ -z "$ENVIRONMENT" ]; then
  usage
fi

REGION="\${AWS_REGION:-eu-west-2}"
PROJECT_NAME="template-terraform-boilerplate"

while [ "\${1:-}" != "" ]; do
  case "$1" in
    --region)
      shift
      REGION="\${1:-}"
      ;;
    --project-name)
      shift
      PROJECT_NAME="\${1:-}"
      ;;
    *)
      echo "Unknown arg: $1"
      usage
      ;;
  esac
  shift || true
done

ENV_DIR="$ROOT_DIR/env/$ENVIRONMENT"
if [ ! -d "$ENV_DIR" ]; then
  echo "Environment folder not found: $ENV_DIR"
  echo "Create it under infra/env/ and try again."
  exit 1
fi

if ! command -v aws >/dev/null 2>&1; then
  echo "AWS CLI is required."
  exit 1
fi

ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text 2>/dev/null || true)"
CALLER_ARN="$(aws sts get-caller-identity --query Arn --output text 2>/dev/null || true)"

if [ -z "$ACCOUNT_ID" ] || [ "$ACCOUNT_ID" = "None" ]; then
  echo "Could not determine AWS account ID. Are you authenticated?"
  exit 1
fi

STATE_BUCKET="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tfstate"
LOCK_TABLE="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tflocks"
STATE_KEY="\${PROJECT_NAME}/\${ENVIRONMENT}/terraform.tfstate"

cat > "$ENV_DIR/backend.hcl" <<EOF
bucket         = "$STATE_BUCKET"
key            = "$STATE_KEY"
region         = "$REGION"
dynamodb_table = "$LOCK_TABLE"
encrypt        = true
EOF

echo "Wrote $ENV_DIR/backend.hcl"
echo "Account:    $ACCOUNT_ID"
echo "Caller ARN: \${CALLER_ARN:-<unknown>}"`;

const awsConfigExample = `# ~/.aws/config
#
# This file stores AWS CLI profile configuration (not secret credentials).
# Terraform and the AWS CLI both read these profiles, so switching context locally
# can be as simple as setting AWS_PROFILE=<aws-account>.
#
# The profile name can match your infra/env/<aws-account>/ folder name.

[default]
region = eu-west-2

[profile <aws-account-a>]
region = eu-west-2
role_arn = arn:aws:iam::111111111111:role/TerraformExecutionRole
source_profile = default

[profile <aws-account-b>]
region = eu-west-2
role_arn = arn:aws:iam::222222222222:role/TerraformExecutionRole
source_profile = default`;


const awsCredentialsExample = `# ~/.aws/credentials
#
# This file stores credential material for profiles.
# In many teams, "base" is an AWS SSO profile instead of static keys.
# This example uses placeholders so you can see the shape.
#
# "default" credentials are used when AWS_PROFILE isn't set.
# In many teams this is replaced by AWS SSO rather than static keys.

[default]
aws_access_key_id = YOUR_ACCESS_KEY_ID
aws_secret_access_key = YOUR_SECRET_ACCESS_KEY`;

const scriptUseEnv = `#!/usr/bin/env bash
set -euo pipefail

# use-env.sh
# - Switches local AWS context by setting AWS_PROFILE=<environment>.
# - Use with "source" so it persists in your current shell session.
#
# Usage:
#   source infra/scripts/use-env.sh <environment>
#
# Notes:
# - Assumes AWS profiles exist in ~/.aws/config
# - Sets region defaults (can be overridden per shell)

ENVIRONMENT="\${1:-}"
if [ -z "$ENVIRONMENT" ]; then
  echo "Usage: source infra/scripts/use-env.sh <environment>"
  return 1 2>/dev/null || exit 1
fi

export AWS_PROFILE="$ENVIRONMENT"

export AWS_REGION="\${AWS_REGION:-eu-west-2}"
export AWS_DEFAULT_REGION="\${AWS_DEFAULT_REGION:-$AWS_REGION}"

echo "Switched AWS context"
echo "AWS_PROFILE=$AWS_PROFILE"
echo "AWS_REGION=$AWS_REGION"
echo ""
echo "Next:"
echo "  bash infra/scripts/whoami.sh"
echo "  cd infra/env/$ENVIRONMENT"
echo "  bash ../../scripts/write-backend-hcl.sh $ENVIRONMENT --region $AWS_REGION"
echo "  terraform init -backend-config=backend.hcl"`;

const scriptWhoAmI = `#!/usr/bin/env bash
set -euo pipefail

# whoami.sh
# - Prints the current AWS identity (account + principal ARN).
# - Safety check before running plan/apply.
#
# Usage:
#   bash infra/scripts/whoami.sh

if ! command -v aws >/dev/null 2>&1; then
  echo "aws CLI is required for this script."
  echo "Run: bash infra/scripts/prereqs.sh"
  exit 1
fi

if command -v jq >/dev/null 2>&1; then
  aws sts get-caller-identity | jq
else
  aws sts get-caller-identity
fi`;

const terraformRoleTrustPolicyExample = `{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowAssumeFromTrustedPrincipal",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::111111111111:role/YourTrustedRoleOrUser"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}`;

const backendExample = `/*
backend.hcl (generated by the CLI)
- Not committed to Git.
- Injected at init time so backend values stay account-specific and portable.
*/

bucket         = "<state-bucket-name>"
key            = "template-terraform-boilerplate/<aws-account>/terraform.tfstate"
region         = "eu-west-2"
dynamodb_table = "<lock-table-name>"
encrypt        = true
role_arn       = "arn:aws:iam::<account-id>:role/<terraform-execution-role>"`;

const stateKeyConvention = `/*
State key convention (recommended)
- Keep state keys predictable and scoped to the deployable root folder.
- A common pattern is:
  <project>/<aws-account>/terraform.tfstate

Example:
- template-terraform-boilerplate/<aws-account>/terraform.tfstate
*/`;


const githubTerraformWorkflow = `name: Terraform

on:
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment folder under infra/env/"
        required: true
        type: string

permissions:
  id-token: write
  contents: read
  pull-requests: write

concurrency:
  group: terraform-\${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: "eu-west-2"

jobs:
  plan:
    name: Validate + Plan (\${{ matrix.environment }})
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    environment: \${{ matrix.environment }}

    strategy:
      fail-fast: false
      matrix:
        # Add more env folders here as you create them:
        environment: [sandbox]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: \${{ secrets.AWS_ROLE_ARN }}
          aws-region: \${{ env.AWS_REGION }}

      - name: Make scripts executable
        run: chmod +x infra/scripts/*.sh

      - name: Install tflint
        run: |
          set -euo pipefail
          curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash
          tflint --version

      - name: Validate (fmt check + validate + tflint)
        run: bash infra/scripts/validate.sh \${{ matrix.environment }}

      - name: Generate backend.hcl
        shell: bash
        run: |
          set -euo pipefail

          ENVIRONMENT="\${{ matrix.environment }}"
          REGION="\${{ env.AWS_REGION }}"
          PROJECT_NAME="template-terraform-boilerplate"

          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"

          STATE_BUCKET="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tfstate"
          LOCK_TABLE="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tflocks"
          STATE_KEY="\${PROJECT_NAME}/\${ENVIRONMENT}/terraform.tfstate"

          ENV_DIR="infra/env/\${ENVIRONMENT}"
          mkdir -p "\${ENV_DIR}"

          cat > "\${ENV_DIR}/backend.hcl" <<EOF
          bucket         = "\${STATE_BUCKET}"
          key            = "\${STATE_KEY}"
          region         = "\${REGION}"
          dynamodb_table = "\${LOCK_TABLE}"
          encrypt        = true
          EOF

          echo "Wrote \${ENV_DIR}/backend.hcl"

      - name: Plan
        run: bash infra/scripts/plan.sh \${{ matrix.environment }}

      - name: Upload plan artifact
        uses: actions/upload-artifact@v4
        with:
          name: tfplan-\${{ matrix.environment }}
          path: infra/env/\${{ matrix.environment }}/tfplan
          if-no-files-found: error

  apply:
    name: Apply (\${{ matrix.environment }})
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    environment: \${{ matrix.environment }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: \${{ secrets.AWS_ROLE_ARN }}
          aws-region: \${{ env.AWS_REGION }}

      - name: Make scripts executable
        run: chmod +x infra/scripts/*.sh

      - name: Install tflint
        run: |
          set -euo pipefail
          curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash
          tflint --version

      - name: Validate (fmt check + validate + tflint)
        run: bash infra/scripts/validate.sh \${{ matrix.environment }}

      - name: Generate backend.hcl
        shell: bash
        run: |
          set -euo pipefail

          ENVIRONMENT="\${{ matrix.environment }}"
          REGION="\${{ env.AWS_REGION }}"
          PROJECT_NAME="template-terraform-boilerplate"

          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"

          STATE_BUCKET="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tfstate"
          LOCK_TABLE="\${PROJECT_NAME}-\${ACCOUNT_ID}-\${REGION}-tflocks"
          STATE_KEY="\${PROJECT_NAME}/\${ENVIRONMENT}/terraform.tfstate"

          ENV_DIR="infra/env/\${ENVIRONMENT}"
          mkdir -p "\${ENV_DIR}"

          cat > "\${ENV_DIR}/backend.hcl" <<EOF
          bucket         = "\${STATE_BUCKET}"
          key            = "\${STATE_KEY}"
          region         = "\${REGION}"
          dynamodb_table = "\${LOCK_TABLE}"
          encrypt        = true
          EOF

          echo "Wrote \${ENV_DIR}/backend.hcl"

      - name: Plan
        run: bash infra/scripts/plan.sh \${{ matrix.environment }}

      - name: Apply
        run: bash infra/scripts/apply.sh \${{ matrix.environment }}`;

const bootstrapOverview = `/*
Bootstrap (why it exists)
- Terraform needs a backend for remote state (S3 + DynamoDB locking).
- But Terraform cannot safely create its own backend on the very first run.
- So this template ships a tiny CLI that bootstraps the prerequisites:
  - S3 bucket (state storage)
  - DynamoDB table (state locking)
  - IAM execution role (assumed locally or by CI)
  - GitHub OIDC role for Actions
- It then generates infra/backend.hcl so terraform init works immediately.
*/`;

const bootstrapCommands = `# from repo root

# 1) Bootstrap AWS prerequisites into the currently authenticated account
#    (switch AWS_PROFILE / credentials to target a different account)
npm run tf:bootstrap -- --target <aws-account> --region eu-west-2

# 2) Initialise Terraform using the generated backend config
cd infra/env/<aws-account>
terraform init -backend-config=../../backend.hcl

# 3) Validate / plan / apply using the scripts
bash ../../scripts/validate.sh <aws-account>
bash ../../scripts/plan.sh <aws-account>
bash ../../scripts/apply.sh <aws-account>`;

const backendHclExample = `# infra/backend.hcl (generated by the CLI)
bucket         = "<state-bucket-name>"
key            = "template-terraform-boilerplate/<aws-account>/terraform.tfstate"
region         = "eu-west-2"
dynamodb_table = "<lock-table-name>"
encrypt        = true
role_arn       = "arn:aws:iam::<account-id>:role/<terraform-execution-role>"`;

const backendTfRecommended = `/*
backend.tf (environment root)
- Keep backend config out of Git.
- Backend values cannot use variables, so this file stays minimal.
- You pass real values via -backend-config=../../backend.hcl (generated by CLI).
*/

terraform {
  backend "s3" {}
}`;

const namingAndSingleAccountNote = `/*
Template note: single-account bootstrap + naming
- This template's CLI bootstraps prerequisites into the AWS account you're currently authenticated to.
- Want another account? Switch AWS_PROFILE (or pass --profile) and run the command again.
- Bucket/table/role names in this post are examples only.
  The CLI can generate safe defaults, and you can override names explicitly when you want full control.
*/`;

const IaCTerraform = () => {
  useEffect(() => {
    Analytics.event('blog_opened', { slug: 'infrastructure-as-code-with-terraform' });
  }, []);

  return (
    <PageWrapper>
      <PostTopBar>
        <BackButton />
      </PostTopBar>

      <AnimatedPostContainer>
        <HeaderRow>
          <PageTitle>Infrastructure as Code (IaC) with Terraform</PageTitle>
          <IconWrapper>
            <HeaderIcon>
              <TerraformSVG />
            </HeaderIcon>
          </IconWrapper>
        </HeaderRow>

        <Paragraph>
          In this post, we're going to build a Terraform template repo and work through the core workflow (init, plan, apply).
          We'll cover how to structure a project sensibly, and how to take the same setup from local development into CI and
          multiple environments.
        </Paragraph>

        <Paragraph>
          We'll start by organising the repo in a way that's easy to understand, then introduce Terraform concepts as they become relevant.
        </Paragraph>

        <Paragraph>
          Here is a link to the boilerplate that works locally and sets you up nicely for CI and multiple environments - <TextLink
            href="https://github.com/heyitsmeharv/template-terraform-boilerplate"
            target="_blank"
            rel="noreferrer"
          >
            template-terraform-boilerplate
          </TextLink>
        </Paragraph>

        <SectionHeading>What is Terraform?</SectionHeading>

        <Paragraph>
          Terraform is an <Strong>Infrastructure as Code</Strong> tool. It's how teams create repeatable environments,
          manage changes safely, and keep infrastructure consistent across accounts and regions.
        </Paragraph>

        <SectionHeading>Install Terraform (and verify)</SectionHeading>

        <Paragraph>
          Terraform is distributed as a single CLI tool. The easiest way to install it is usually through your system package manager, but you can also
          download the binary directly if you prefer.
        </Paragraph>

        <Paragraph>
          If you want the most up-to-date steps for your OS, these are the official docs I'd follow:
          {" "}
          <TextLink
            href="https://developer.hashicorp.com/terraform/install"
            target="_blank"
            rel="noreferrer"
          >
            Terraform install page
          </TextLink>
          {" "}
          and
          {" "}
          <TextLink
            href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli"
            target="_blank"
            rel="noreferrer"
          >
            Install Terraform CLI tutorial
          </TextLink>
          .
        </Paragraph>

        <SubSectionHeading>Verify the CLI is working</SubSectionHeading>
        <Paragraph>
          Once Terraform is installed, run the commands below. If they work, you're ready to move on.
        </Paragraph>

        <CodeBlockWithCopy code={verifyTf} />

        <SectionHeading>Repository Structure</SectionHeading>

        <Paragraph>
          Before we write any Terraform, we're going to agree on a structure that stays easy to navigate as the project grows. Everything Terraform-related
          lives under <InlineHighlight>infra/</InlineHighlight>, so the rest of the repo (app code, docs, tooling) can evolve independently.
        </Paragraph>

        <Paragraph>
          This layout separates two concerns: <Strong>environment roots</Strong> (where we actually run Terraform) and <Strong>modules</Strong> (reusable
          building blocks). That separation is what keeps your repo from turning into one huge folder.
        </Paragraph>

        <SubSectionHeading>High-level layout</SubSectionHeading>
        <CodeBlockWithCopy code={terraformFolderTree} />

        <SubSectionHeading>.github/workflows/</SubSectionHeading>
        <Paragraph>
          This is where your CI workflow lives. Later on, we'll add a workflow that runs <InlineHighlight>terraform fmt</InlineHighlight>,{" "}
          <InlineHighlight>validate</InlineHighlight> and <InlineHighlight>plan</InlineHighlight> on pull requests, and only runs{" "}
          <InlineHighlight>apply</InlineHighlight> on merges to main. Keeping workflows next to the code makes your infrastructure changes reviewable and
          repeatable. I have a separate blog post going through GitHub's CI/CD which you can find here if you want a rundown on how that works{" "}
          <TextLink href="/blog/github-ci-cd">GitHub CI/CD</TextLink>.
        </Paragraph>

        <SubSectionHeading>infra/env/</SubSectionHeading>
        <Paragraph>
          Each folder under <InlineHighlight>infra/env/</InlineHighlight> is the deployable Terraform root. This is the folder you{" "}
          <Strong>cd into</Strong> when you run Terraform commands for that environment.
        </Paragraph>

        <Paragraph>
          Typically you create a folder for each AWS account you deploy into. Let's run through each file
          you would find in the environment folder:
        </Paragraph>

        <CodeCarousel
          items={[
            {
              title: "main.tf",
              description: "Environment entry point where you wire modules together and keep the overall intent readable.",
              code: envMainTf,
            },
            {
              title: "variables.tf",
              description: "Typed inputs for the environment so configuration stays explicit and mistakes are caught early.",
              code: envVariablesTf,
            },
            {
              title: "env.tfvars",
              description: "Environment-specific values so the Terraform code can stay the same across environments.",
              code: envTfvars,
            },
            {
              title: "providers.tf",
              description: "Provider configuration for this environment (region/account context), inherited by modules.",
              code: envProvidersTf,
            },
            {
              title: "outputs.tf",
              description: "The important values you want after apply (names, IDs, URLs) without digging through state.",
              code: envOutputsTf,
            },
            {
              title: "backend.tf",
              description: "Minimal backend block. Real state settings are injected via infra/backend.hcl at init time.",
              code: envBackendTf,
            },
          ]}
        />

        <SubSectionHeading>infra/modules/</SubSectionHeading>
        <Paragraph>
          Modules are reusable pieces of infrastructure you can wire together from an environment root. If an environment folder starts to feel like a
          long list of resources, it's usually better to break that up into multiple modules.
        </Paragraph>

        <Paragraph>
          Notice how we pass attributes from the main.tf in the environment folder to be used as variables inside our module.
        </Paragraph>

        <CodeCarousel
          items={[
            {
              title: "main.tf",
              description: "The resources this module creates (keep modules small and single-purpose).",
              code: moduleMainTf,
            },
            {
              title: "variables.tf",
              description: "Inputs the module needs so it stays reusable across environments.",
              code: moduleVariablesTf,
            },
            {
              title: "outputs.tf",
              description: "What the module returns so other parts of the system can connect to it cleanly.",
              code: moduleOutputsTf,
            }
          ]}
        />

        <SubSectionHeading>infra/scripts/</SubSectionHeading>

        <Paragraph>
          These scripts are optional, but they make local development feel the same as CI. GitHub Actions will run the workflow end-to-end, but when you're
          working locally it's still useful to have a consistent way to format, validate, plan, and apply - especially once you introduce multiple environments.
        </Paragraph>

        <Paragraph>
          The main idea is that whether you're working locally or in CI, you're running the same steps in the same order:{" "}
          <InlineHighlight>fmt</InlineHighlight> → <InlineHighlight>validate</InlineHighlight> → <InlineHighlight>plan</InlineHighlight> →{" "}
          <InlineHighlight>apply</InlineHighlight>.
        </Paragraph>

        <Paragraph>
          I will go through the scripts usage more thoroughly in it's own section <Strong>(Local Development)</Strong> as to not to distract from
          the purpose of this topic.
        </Paragraph>

        <SubSectionHeading>repo-level files</SubSectionHeading>
        <Paragraph>
          <Strong>.gitignore</Strong> should exclude .terraform state files, and plan files so you never commit
          sensitive or noisy artifacts.
        </Paragraph>
        <Paragraph>
          <Strong>README.md</Strong> becomes your "how to run this repo" entry point: what it deploys, how environments work, and the basic commands.
        </Paragraph>
        <Paragraph>
          <Strong>package.json</Strong> is optional, but if you're already using Node tooling for your projects it can be a nice place to standardise
          scripts (for example: running Terraform scripts, formatting, linting, and CI helpers) in one familiar interface.
        </Paragraph>

        <SectionHeading>Local Development</SectionHeading>
        <Paragraph>
          We touched on the topics of scripts above in the Repository Structure about how they could be used to help run your terraform locally. What is
          also handy is the ability to maintain a local setup when working in multiple environments.
        </Paragraph>

        <Paragraph>
          Locally, we'll use{" "} <Strong>AWS profiles per environment</Strong> so switching between AWS accounts is explicit and low-risk.
        </Paragraph>

        <SubSectionHeading>AWS profiles per environment</SubSectionHeading>

        <Paragraph>
          The approach is simple: create one AWS CLI profile per environment in{" "}
          <InlineHighlight>~/.aws/config</InlineHighlight>, and store your base credentials in{" "}
          <InlineHighlight>~/.aws/credentials</InlineHighlight>.
          Terraform and the AWS CLI both understand these files. The AWS docs cover the file locations and formats in detail:
          {" "}
          <TextLink href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html" target="_blank" rel="noreferrer">
            AWS CLI config & credentials files
          </TextLink>
          {" "}
          and
          {" "}
          <TextLink href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html" target="_blank" rel="noreferrer">
            Configuring the AWS CLI
          </TextLink>
          .
        </Paragraph>

        <Paragraph>
          In a team setup, you'll typically assume a Terraform role per environment/account. AWS roles are split into two parts: a{" "}
          <InlineHighlight>trust policy</InlineHighlight> (who can assume the role) and a{" "}
          <InlineHighlight>permissions policy</InlineHighlight> (what the role can do).
        </Paragraph>

        <Paragraph>
          If you need a refresher, the best starting point:
          {" "}
          <TextLink href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank" rel="noreferrer">
            IAM roles overview
          </TextLink>
          {" "}
          and
          {" "}
          <TextLink href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-custom.html" target="_blank" rel="noreferrer">
            Create a role with a custom trust policy
          </TextLink>
          .
          {" "}
          Alternatively you can look at my
          {" "}
          <TextLink href="https://www.heyitsmeharv.com/blog/aws-identity-access-management" target="_blank" rel="noreferrer">
            AWS Identity and Access Management (IAM) blog post
          </TextLink>
          .
        </Paragraph>

        <CodeCarousel
          items={[
            {
              title: "~/.aws/config",
              description: `This file stores AWS CLI profile configuration (not secret credentials).`,
              code: awsConfigExample,
            },
            {
              title: "~/.aws/credentials",
              description: "This file stores credentials for profiles. This file should never be shared or else you risk losing your account!",
              code: awsCredentialsExample,
            },
            {
              title: "Terraform role trust policy",
              description: `This is a simple example of a trust policy that allows a specific principal to assume the role.
              The actual principal will depend on your setup (an IAM user, an SSO role, or a role in another account). The key idea is: trust policy controls 
              who can assume, permissions policy controls what they can do.`,
              code: terraformRoleTrustPolicyExample,
            }
          ]}
        />

        <SubSectionHeading>Scripts</SubSectionHeading>
        <Paragraph>
          These scripts assume you're using named AWS profiles. The main one you'll use is{" "}
          <InlineHighlight>use-env.sh</InlineHighlight>, which sets <InlineHighlight>AWS_PROFILE</InlineHighlight> for your shell session.
          From there, Terraform commands run in the correct account/role context.
        </Paragraph>

        <CodeCarousel
          items={[
            {
              title: "prereqs.sh",
              description: "Sanity-checks your local Git Bash setup by verifying required tools are available on PATH (terraform, aws, jq, tflint, node, npm). Fails fast with a clear fix hint so you don't waste time debugging 'command not found' issues later.",
              code: scriptPreReqs,
            },
            {
              title: "use-env.sh",
              description: "Switches AWS context locally by setting AWS_PROFILE to match a environment name.",
              code: scriptUseEnv,
            },
            {
              title: "whoami.sh",
              description: "Prints your active AWS identity (account/role) so you don't plan/apply in the wrong place.",
              code: scriptWhoAmI,
            },
            {
              title: "bootstrap-state.sh",
              description: "Bootstraps the AWS prerequisites Terraform needs before the first init: creates the remote state S3 bucket (versioning + encryption + public access block) and the DynamoDB lock table, then sets up GitHub Actions OIDC (provider + role) and writes backend.hcl so your environment can run terraform init immediately against remote state.",
              code: scriptBootstrapState,
            },
            {
              title: "write-backend-hcl.sh",
              description: "Generates an env-bound backend.hcl inside infra/env/<environment>/ using your current AWS identity. It derives the account ID via sts, builds deterministic S3 + DynamoDB backend names, and writes the exact config Terraform needs for remote state (bucket, key, region, lock table, encryption) without committing any backend values to Git.",
              code: scriptWriteBackendHCL,
            },
            {
              title: "fmt.sh",
              description: "Formats Terraform code under infra so the code stay clean.",
              code: scriptFmt,
            },
            {
              title: "validate.sh",
              description: "The local quality gate (fmt check + validate + tflint) before you generate a plan.",
              code: scriptValidate,
            },
            {
              title: "plan.sh",
              description: "Creates a saved plan file using env.tfvars so changes can be reviewed.",
              code: scriptPlan,
            },
            {
              title: "apply.sh",
              description: "Applies the saved plan file so you deploy exactly what you planned.",
              code: scriptApply,
            },
          ]}
        />

        <Paragraph>
          Once we introduce GitHub Actions, the workflow will run these same steps automatically. The scripts are just there to keep your local workflow
          consistent and safe, especially when you're switching environments.
        </Paragraph>

        <SubSectionHeading>Backend conventions</SubSectionHeading>

        <Paragraph>
          You'll see backend config kept in a dedicated <InlineHighlight>backend.tf</InlineHighlight> at the environment root. I like this because it makes state
          behaviour explicit per environment, and keeps it separate from provider configuration.
        </Paragraph>

        <Paragraph>
          The key detail is the <InlineHighlight>key</InlineHighlight>. That's the path inside the bucket where the state file lives. Use a predictable convention
          so it's always obvious which environment you're looking at.
        </Paragraph>

        <CodeBlockWithCopy code={stateKeyConvention} />

        <SubSectionHeading>Backend example</SubSectionHeading>

        <Paragraph>
          This is the standard AWS pattern: S3 stores the state file, and DynamoDB provides a lock so concurrent applies don't collide.
          In this template, the real backend values live in <InlineHighlight>infra/backend.hcl</InlineHighlight> (generated by the CLI).
        </Paragraph>
        <CodeBlockWithCopy code={backendExample} />

        <SubSectionHeading>What remote state solves</SubSectionHeading>

        <TextList>
          <TextListItem>
            <Strong>Shared source of truth</Strong> - your laptop and CI both read/write the same state, so you don't end up with competing copies.
          </TextListItem>
          <TextListItem>
            <Strong>Locking</Strong> - prevents two applies happening at once, which is one of the fastest ways to corrupt state.
          </TextListItem>
          <TextListItem>
            <Strong>Environment isolation</Strong> - each environment gets its own separate state file, even if they share the same module code.
          </TextListItem>
        </TextList>

        <SubSectionHeading>What changes locally when you enable a backend</SubSectionHeading>
        <Paragraph>
          Once <InlineHighlight>backend.tf</InlineHighlight> is present, <InlineHighlight>terraform init</InlineHighlight> will initialise the backend and move your
          state into it. From that point on, <InlineHighlight>plan</InlineHighlight> and <InlineHighlight>apply</InlineHighlight> operate against remote state -
          which is exactly what you want for CI.
        </Paragraph>

        <Paragraph>
          Now we've finished looking at local development, we can now look into integrating the same flow into GitHub Actions,
          but putting processes in place so that pull requests generate plans automatically, and main branch merges are the only thing that can apply the terraform.
        </Paragraph>

        <SectionHeading>Bootstrap AWS prerequisites</SectionHeading>

        <Paragraph>
          Remote state is non-negotiable once you introduce CI or multiple environments. The catch is that Terraform can't cleanly
          create its own backend on the very first run - because it needs state storage before it can manage anything.
        </Paragraph>

        <Paragraph>
          So this template includes a small bootstrap tool under <InlineHighlight>bootstrap/</InlineHighlight>. It creates the AWS prerequisites
          (S3 + DynamoDB + TerraformExecutionRole) and generates a backend config file so <InlineHighlight>terraform init</InlineHighlight> works immediately.
        </Paragraph>

        <CodeBlockWithCopy code={bootstrapOverview} />

        <SubSectionHeading>What gets created</SubSectionHeading>
        <TextList>
          <TextListItem>
            <Strong>S3 state bucket</Strong> - versioning enabled, encryption on, and public access blocked.
          </TextListItem>
          <TextListItem>
            <Strong>DynamoDB lock table</Strong> - prevents concurrent applies from corrupting state.
          </TextListItem>
          <TextListItem>
            <Strong>TerraformExecutionRole</Strong> - a consistent role you can assume locally (AWS profiles) and later from CI (OIDC).
          </TextListItem>
        </TextList>

        <SubSectionHeading>Generated backend config</SubSectionHeading>
        <Paragraph>
          The bootstrap command generates <InlineHighlight>infra/backend.hcl</InlineHighlight> (gitignored). We then pass it into
          <InlineHighlight>terraform init</InlineHighlight> so each environment can use remote state without committing account-specific backend values.
        </Paragraph>
        <CodeCarousel
          items={[
            {
              title: "infra/backend.hcl (generated)",
              description: "Real backend values (bucket/table/role) generated per environment.",
              code: backendHclExample,
            },
            {
              title: "backend.tf (environment root)",
              description: "Minimal backend block. Values are injected at init time using backend.hcl.",
              code: backendTfRecommended,
            },
          ]}
        />

        <SubSectionHeading>Run the bootstrap end-to-end</SubSectionHeading>
        <Paragraph>
          These commands assume you are running in Git Bash and have an AWS CLI profile that matches your environment folder name
          (for example: <InlineHighlight>sandbox</InlineHighlight>).
        </Paragraph>
        <CodeBlockWithCopy code={bootstrapCommands} />

        <Paragraph>
          One important template detail: the bootstrap step is intentionally <Strong>single-account</Strong>. It creates resources in
          whichever AWS account your credentials point at. If you want to bootstrap a second account, switch AWS context and run it again.
        </Paragraph>

        <CodeBlockWithCopy code={namingAndSingleAccountNote} />

        <SectionHeading>GitHub Actions: plan on PR, apply on main</SectionHeading>

        <Paragraph>
          Once your repo structure is in place and you've got an execution role per environment, GitHub Actions becomes the glue that makes Terraform feel
          safe and repeatable. The goal is simple:
        </Paragraph>

        <TextList>
          <TextListItem>
            <Strong>Pull requests</Strong> generate plans automatically for each environment, so changes are reviewed like code.
          </TextListItem>
          <TextListItem>
            <Strong>Applies</Strong> are triggered manually with <InlineHighlight>workflow_dispatch</InlineHighlight>, so deployments are always intentional.
          </TextListItem>
          <TextListItem>
            <Strong>Sensitive environments</Strong> should be protected with required reviewers via GitHub Environments.
          </TextListItem>
        </TextList>

        <SubSectionHeading>Authenticating to AWS (OIDC)</SubSectionHeading>

        <Paragraph>
          For CI, the cleanest pattern is to use GitHub's OIDC integration to assume an AWS role with short-lived credentials. That means you don't need to store
          long-lived AWS access keys in GitHub secrets. GitHub and AWS both document this approach, and the{" "}
          <InlineHighlight>aws-actions/configure-aws-credentials</InlineHighlight> action supports it directly.
        </Paragraph>

        <Paragraph>
          The only requirement on the workflow side is granting <InlineHighlight>id-token: write</InlineHighlight> so the job can request an OIDC token.
        </Paragraph>

        <SubSectionHeading>Environment protection for sensitive environments</SubSectionHeading>

        <Paragraph>
          GitHub Environments let you put guardrails around deployments. If an environment is sensitive, you can require reviewers so any apply job targeting that
          environment pauses until someone approves. This gives you a clean manual approval step without custom logic.
        </Paragraph>

        <SubSectionHeading>The workflow</SubSectionHeading>

        <Paragraph>
          The workflow below follows the same shape as our local scripts:
          <InlineHighlight> validate </InlineHighlight> → <InlineHighlight> plan </InlineHighlight> → <InlineHighlight> apply </InlineHighlight>.
          On pull requests, it runs <Strong>validate + plan</Strong> for each environment in the matrix and uploads the plan output as an artifact.
          Applies are triggered manually via <InlineHighlight>workflow_dispatch</InlineHighlight> and run under a GitHub Environment named after the environment.
        </Paragraph>

        <CodeBlockWithCopy code={githubTerraformWorkflow} />

        <Paragraph>
          Under the hood, we rely on <InlineHighlight>hashicorp/setup-terraform</InlineHighlight> to install Terraform on the runner, and we use OIDC auth to assume
          the correct Terraform execution role for each environment.
        </Paragraph>

        <SectionHeading>GitHub repo setup (Environments, approvals, secrets)</SectionHeading>

        <Paragraph>
          The workflow is only half the story. To make Terraform safe in CI, you need a small amount of GitHub repo configuration:
          GitHub Environments (for approvals + scoping secrets) and a place to store the AWS role ARN that GitHub will assume via OIDC.
        </Paragraph>

        <Paragraph>
          In this template, we treat each folder under <InlineHighlight>infra/env/</InlineHighlight> as a deployable environment. In CI, that same environment name is used as a
          GitHub Environment. This keeps everything consistent and makes it harder to apply to the wrong place accidentally.
        </Paragraph>

        <SubSectionHeading>1) Create GitHub Environments to match your aws accounts</SubSectionHeading>

        <Paragraph>
          Create one GitHub Environment per environment. The environment name should match the folder name under <InlineHighlight>infra/env/</InlineHighlight> exactly.
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Go to <Strong>Settings</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            In the left sidebar, click <Strong>Environments</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Click <Strong>New environment</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Create an environment named exactly the same as your environment folder (for example: <InlineHighlight>account-a</InlineHighlight>)
          </IndentedTextListItem>
          <IndentedTextListItem>
            Repeat for each environment folder under <InlineHighlight>infra/env/</InlineHighlight>
          </IndentedTextListItem>
        </IndentedTextList>

        <Paragraph>
          If your workflow uses<InlineHighlight>{'environment: ${{ inputs.environment }}'}</InlineHighlight>, then when you trigger an apply for a environment,
          GitHub automatically scopes secrets and approvals to the matching Environment.
        </Paragraph>

        <SubSectionHeading>2) Add manual approvals for sensitive environments</SubSectionHeading>

        <Paragraph>
          GitHub Environments let you add guardrails around deployments. If a environment is sensitive, you can require reviewers so applies pause until someone approves.
          GitHub calls these <Strong>deployment protection rules</Strong>.
        </Paragraph>

        <Paragraph>
          In your repo:
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Go to <Strong>Settings</Strong> → <Strong>Environments</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Click the environment for your sensitive environment (for example: <InlineHighlight>account-b</InlineHighlight>)
          </IndentedTextListItem>
          <IndentedTextListItem>
            Under <Strong>Deployment protection rules</Strong>, enable <Strong>Required reviewers</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Add yourself (or a team) as a required reviewer
          </IndentedTextListItem>
        </IndentedTextList>

        <Paragraph>
          Now, any job that targets that Environment will pause and wait for approval before it can run.
        </Paragraph>

        <SubSectionHeading>3) Add the AWS role ARN as an Environment secret</SubSectionHeading>

        <Paragraph>
          The workflow needs an IAM role ARN to assume via OIDC. The cleanest pattern is to store this as an{" "}
          <Strong>Environment secret</Strong> so each environment can assume its own role without you hardcoding values in the workflow.
        </Paragraph>

        <Paragraph>
          For each GitHub Environment you created:
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Go to <Strong>Settings</Strong> → <Strong>Environments</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Click the environment that matches your environment folder (for example: <InlineHighlight>account-a</InlineHighlight>)
          </IndentedTextListItem>
          <IndentedTextListItem>
            Under <Strong>Environment secrets</Strong>, click <Strong>Add secret</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Add <InlineHighlight>AWS_ROLE_ARN</InlineHighlight> = <InlineHighlight>arn:aws:iam::...:role/&lt;terraform-execution-role&gt;</InlineHighlight>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Repeat for each target environment with that target's role ARN
          </IndentedTextListItem>
        </IndentedTextList>

        <Paragraph>
          Because the secret name is the same everywhere (<InlineHighlight>AWS_ROLE_ARN</InlineHighlight>), the workflow stays simple and GitHub automatically supplies
          the correct value based on the target environment.
        </Paragraph>

        <SubSectionHeading>4) What it looks like when an apply is waiting for approval</SubSectionHeading>

        <Paragraph>
          When you trigger an apply for a protected target, the workflow run will pause at "Deployment protection rules" and wait. A reviewer can then approve and start
          the waiting job directly from the workflow run page.
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Go to the <Strong>Actions</Strong> tab
          </IndentedTextListItem>
          <IndentedTextListItem>
            Open the running workflow
          </IndentedTextListItem>
          <IndentedTextListItem>
            In <Strong>Deployment protection rules</Strong>, click <Strong>Review deployments</Strong> / <Strong>Start all waiting jobs</Strong> (wording varies)
          </IndentedTextListItem>
        </IndentedTextList>

        <SubSectionHeading>5) Why Workflow permissions must allow OIDC (id-token: write)</SubSectionHeading>

        <Paragraph>
          GitHub Actions can only assume your AWS OIDC role if it's allowed to request an OIDC token. That token is what AWS trusts
          to exchange for short-lived credentials. If your repo/workflow permissions don't allow{" "}
          <InlineHighlight>id-token: write</InlineHighlight>, GitHub can't mint the token, which means{" "}
          <InlineHighlight>aws-actions/configure-aws-credentials</InlineHighlight> has nothing to send to AWS — and the role assumption
          fails with "Credentials could not be loaded".
        </Paragraph>

        <Paragraph>
          In other words: <Strong>id-token: write is the permission that unlocks "allow this workflow run to authenticate to AWS via OIDC."</Strong>
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Without it, GitHub cannot issue an OIDC token for the job
          </IndentedTextListItem>
          <IndentedTextListItem>
            Without the token, AWS will not allow <InlineHighlight>AssumeRoleWithWebIdentity</InlineHighlight>
          </IndentedTextListItem>
          <IndentedTextListItem>
            The workflow then falls back to "no credentials available" and fails early
          </IndentedTextListItem>
        </IndentedTextList>

        <Paragraph>
          Here is how you can make the changes in GitHub:
        </Paragraph>

        <IndentedTextList>
          <IndentedTextListItem>
            Go to <Strong>Settings</Strong> in your GitHub repository
          </IndentedTextListItem>
          <IndentedTextListItem>
            In the left sidebar, open <Strong>Actions</Strong> → <Strong>General</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Scroll to <Strong>Workflow permissions</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Select <Strong>Read and write permissions</Strong>
          </IndentedTextListItem>
          <IndentedTextListItem>
            Ensure <Strong>Allow GitHub Actions to create and approve pull requests</Strong> is enabled if you want PR comments/updates
            (optional, but commonly useful for Terraform plan outputs)
          </IndentedTextListItem>
          <IndentedTextListItem>
            Click <Strong>Save</Strong>
          </IndentedTextListItem>
        </IndentedTextList>

        <SubSectionHeading>6) Sanity check: OIDC permissions</SubSectionHeading>

        <Paragraph>
          For OIDC to work, your workflow must have <InlineHighlight>permissions: id-token: write</InlineHighlight>. Without it, GitHub can't use the OIDC token
          and the AWS credentials step won't be able to assume the role.
        </Paragraph>

        <Paragraph>
          If you see auth errors in CI, the first things to check are:
        </Paragraph>

        <TextList>
          <TextListItem>
            <Strong>The job has id-token: write</Strong> in workflow permissions
          </TextListItem>
          <TextListItem>
            <Strong>The IAM role trust policy</Strong> allows your GitHub repo/org to assume the role via OIDC
          </TextListItem>
          <TextListItem>
            <Strong>The environment secret exists</Strong> (AWS_ROLE_ARN) for the target you're trying to run
          </TextListItem>
        </TextList>

      </AnimatedPostContainer>
    </PageWrapper >
  );
}

export default IaCTerraform;
